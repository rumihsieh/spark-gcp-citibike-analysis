{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf4ac59",
   "metadata": {},
   "source": [
    "# Apache Spark GCP Citibike Analysis #\n",
    "\n",
    "# Citibike Data Engineering with Apache Spark and Google Cloud Platform\n",
    "\n",
    "This project demonstrates large-scale data processing using **Apache Spark (RDD API)** and **Scala**, applied to a ~15GB Citibike dataset stored on **Google Cloud Storage (GCS)**.  \n",
    "It includes data ingestion, preprocessing, time calculations, performance comparisons using different partitioning strategies, and the creation of a custom geospatial partitioner using zipcode polygons.\n",
    "\n",
    "The goal of this notebook is to simulate real-world data engineering tasks involving massive datasets, geo-coordinate handling, and partition optimization for distributed computing systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157cadbc",
   "metadata": {},
   "source": [
    "### Required Files ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5451241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "citibike_data_file = gs://analytics_on_cloud_bucket/data.csv\n",
       "zipcode_geojson_file = gs://analytics_on_cloud_bucket/data/zipcode_coordinates.json\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "gs://analytics_on_cloud_bucket/data/zipcode_coordinates.json"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val citibike_data_file = \"gs://analytics_on_cloud_bucket/data.csv\"\n",
    "\n",
    "val zipcode_geojson_file = \"gs://analytics_on_cloud_bucket/data/zipcode_coordinates.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1874363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------------------+--------------------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "|         ride_id|rideable_type|          started_at|            ended_at|start_station_name|start_station_id|    end_station_name|end_station_id|start_lat| start_lng|  end_lat|   end_lng|member_casual|\n",
      "+----------------+-------------+--------------------+--------------------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "|7FB44ACE772F2B5E| classic_bike|2020-01-16 11:29:...|2020-01-16 11:38:...|E 47 St & Park Ave|         6584.12|     5 Ave & E 63 St|       6904.06|40.755103|-73.974987|40.766368|-73.971518|       member|\n",
      "|514091422D32E0D4| classic_bike|2020-01-15 12:49:...|2020-01-15 13:23:...|Carmine St & 6 Ave|         5763.03| Mott St & Prince St|       5561.04|40.730386| -74.00215| 40.72318|  -73.9948|       member|\n",
      "|C094CBA14014648D| classic_bike|2020-01-02 08:54:...|2020-01-02 09:05:...|Carmine St & 6 Ave|         5763.03|Allen St & Riving...|       5414.06|40.730386| -74.00215|40.720196|-73.989978|       member|\n",
      "|6675D4453C4DBFE4| classic_bike|2020-01-24 12:58:...|2020-01-24 13:08:...|Carmine St & 6 Ave|         5763.03|Allen St & Riving...|       5414.06|40.730386| -74.00215|40.720196|-73.989978|       member|\n",
      "|FE65CE25E01D48B7| classic_bike|2020-01-16 13:19:...|2020-01-16 13:29:...|Carmine St & 6 Ave|         5763.03|Allen St & Riving...|       5414.06|40.730386| -74.00215|40.720196|-73.989978|       member|\n",
      "+----------------+-------------+--------------------+--------------------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [ride_id: string, rideable_type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ride_id: string, rideable_type: string ... 11 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").csv(citibike_data_file)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9419d2",
   "metadata": {},
   "source": [
    "## We define several helper functions used throughout the analysis:\n",
    "\n",
    "- `timer`: measure execution time of a code block  \n",
    "- `time_difference`: compute duration between timestamps  \n",
    "- `get_lat_lon`: extract latitude and longitude safely  \n",
    "- `read_zipcode_polygons`: read zipcode geojson and parse polygon boundaries  \n",
    "- `find_zipcode`: identify zipcode using point-in-polygon lookup  \n",
    "\n",
    "These abstractions make the rest of the notebook cleaner and modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c7bd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timer: [R](block: => R)R\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//times the execution of the enclosed block\n",
    "def timer[R](block: => R): R = { //R is the placeholder return type for the block\n",
    "  val t0 = System.nanoTime() //Returns the system time\n",
    "  val result = block //executes the block and stores the result\n",
    "  val t1 = System.nanoTime()\n",
    "  println(s\"Elapsed time: ${(t1 - t0)/1e9} seconds\") //the time difference \n",
    "  result //return the result\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74df9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.02721878 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1784293664"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Example\n",
    "\n",
    "timer {\n",
    "    val seq = (1 to 1000000).toSeq\n",
    "    seq.reduce(_+_)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f0198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_difference: (t1: String, t2: String)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//This function returns the difference between two times \n",
    "//The format of the times is the citibike format\n",
    "//I.e., \"yyyy-MM-dd HH:mm:ss.SSS\" \n",
    "\n",
    "def time_difference(t1:String, t2:String) = {\n",
    "    import java.time.LocalDateTime\n",
    "    import java.time.format.DateTimeFormatter\n",
    "    import java.time.Duration    \n",
    "\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    "    // Parse into LocalDateTime objects\n",
    "    val startTime = LocalDateTime.parse(t1, formatter)\n",
    "    val endTime   = LocalDateTime.parse(t2, formatter)\n",
    "    \n",
    "    //Find the difference\n",
    "    val diffSeconds = Duration.between(startTime, endTime).toMillis / 1000.0\n",
    "    \n",
    "    //Return the difference\n",
    "    diffSeconds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c1bfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389.689"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Example\n",
    "time_difference(\"2020-01-16 11:29:00.511\",\"2020-01-16 11:35:30.200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529daa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invalid_lat_lon = (0.0,0.0)\n",
       "valid_lat_lon = (40.755103,-73.974987)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "get_lat_lon: (lat: Any, lon: Any)(Double, Double)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(40.755103,-73.974987)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//The latitude and longitude data in the file needs to be converted into Double, Double\n",
    "//However, there is some bad data in it\n",
    "//This function returns (Double,Double) if the data is good and (0.0,0.0) if the data is bad\n",
    "def get_lat_lon(lat: Any, lon: Any): (Double,Double) = {\n",
    "    try {\n",
    "       (lat.toString.toDouble,lon.toString.toDouble)\n",
    "    } catch {\n",
    "        case e: Throwable => (0.0,0.0)\n",
    "    }\n",
    "}\n",
    "val invalid_lat_lon = get_lat_lon(\"aa\", -73.974987)\n",
    "val valid_lat_lon = get_lat_lon(\"40.755103\",-73.974987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3831913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_zipcode_polygons: (file: String)org.apache.spark.rdd.RDD[(String, Int, Array[(Double, Double)])]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Zipcode data is in the geojson file zipcode_coordinates.json\n",
    "//The file contains information about the zipcode including the polygon of coordinates that defines each zipcode\n",
    "//This function reads the file and returns an rdd containing:\n",
    "/* \n",
    "each zipcode, a unique id for each zipcode (0...261), and an Array containing the coordiates of the polygon\n",
    "*/\n",
    "def read_zipcode_polygons(file: String) = {\n",
    "    val df = spark.read.json(file)\n",
    "\n",
    "    // convert to RDD \n",
    "    val rdd = df.rdd.map { row =>\n",
    "      val zipcode = row.getAs[String](\"zipcode\")\n",
    "      val coords = row.getAs[Seq[Seq[Seq[Double]]]](\"coords\")\n",
    "      (zipcode, coords)\n",
    "    }\n",
    "\n",
    "    // now take a few\n",
    "\n",
    "    val polygon_rdd = rdd.map(t=>(t._1,t._2(0).toArray.map(a=>a.toArray).map(a=>(a(0),a(1)))))\n",
    "    .zipWithUniqueId()\n",
    "    .map(t => (t._1._1,t._2.toInt,t._1._2))\n",
    "    polygon_rdd\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3d147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r1 = -1\n",
       "r2 = 105\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "find_zipcode: (p: (String, Int, Array[(Double, Double)]), c: (Double, Double))Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//given a (lon,lat) and a polygon this function returns the zipcode id if the coordinates\n",
    "//. are in the polygon or -1 if they are not\n",
    "def find_zipcode(\n",
    "    p: (String, Int, Array[(Double, Double)]), //The Polygon\n",
    "    c: (Double, Double)                        //The coordinates (lat, lon)\n",
    "): Int = {\n",
    "  val (zipcode, id, polygon) = p\n",
    "  val (x, y) = c\n",
    "\n",
    "  var inside = false //start by assuming it is not in the polygon\n",
    "  var j = polygon.length - 1\n",
    "\n",
    "  for (i <- polygon.indices) {\n",
    "    val (xi, yi) = polygon(i)\n",
    "    val (xj, yj) = polygon(j)\n",
    "\n",
    "    // Check if the ray crosses the edge\n",
    "    val intersects =\n",
    "      ((yi > y) != (yj > y)) &&\n",
    "      (x < (xj - xi) * (y - yi) / (yj - yi + 1e-12) + xi)\n",
    "\n",
    "    if (intersects) inside = !inside\n",
    "    j = i\n",
    "  }\n",
    "\n",
    "  if (inside) id else -1 //if it is inside the polygon, return the id (partition number)\n",
    "}\n",
    "\n",
    "val r1 = find_zipcode( //random zipcode match (should return -1)\n",
    "(\"11372\",0,Array((-73.86942457284177,40.74915687096788), (-73.89143129977276,40.74684466041932), \n",
    "(-73.89507143240859,40.746465470812154), (-73.8961873786782,40.74850942518088), \n",
    "(-73.8958395418514,40.74854687570604), (-73.89525242774397,40.748306609450246), \n",
    "(-73.89654041085562,40.75054199814359), (-73.89579868613829,40.75061972133262), \n",
    "(-73.89652230661434,40.75438879610903), (-73.88164812188481,40.75595161704187), \n",
    "(-73.87221855882478,40.75694324806748), (-73.87167992356792,40.75398717439604), \n",
    "(-73.8720704651389,40.753862007052064), (-73.86942457284177,40.74915687096788))),\n",
    "    (40.766368, -73.971518)\n",
    ")\n",
    "\n",
    "\n",
    "val r2 = find_zipcode( //Times Square - should return 105\n",
    "(\"10036\",105,Array((-73.98134106844775,40.758645323851304), (-73.9781160605814,40.7572844526636), (-73.98088709157031,40.753481009691896), (-73.98411754582567,40.754842071375435), (-73.98459583212677,40.754176960006426), (-73.99273700488898,40.75760450929245), (-73.99395968106315,40.758252769648045), (-74.00170164222186,40.76138075542977), (-74.00116297247455,40.76202186185266), (-73.99939972778823,40.76337781582372), (-73.99894124905622,40.76387032527473), (-73.99773597314044,40.765557990235976), (-73.98134106844775,40.758645323851304))),\n",
    "    (-73.985130, 40.758896)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95793271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_rdd = ParallelCollectionRDD[14] at parallelize at <console>:25\n",
       "new_sample_rdd = MapPartitionsRDD[15] at map at <console>:26\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Array(1, jack, 37.2, 74.4), Array(2, Jill, 23.1, 46.2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//How to add an element to an array\n",
    "val sample_rdd = sc.parallelize(Array(Array(1,\"jack\",37.2),Array(2,\"Jill\",23.1)))\n",
    "val new_sample_rdd = sample_rdd.map(a => a:+ a(2).toString.toDouble*2)\n",
    "new_sample_rdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75d744",
   "metadata": {},
   "source": [
    "### Scala help: Type safety ###\n",
    "* When you construct a type implicitly, i.e. you don't explicitly give a type, scala tries to figure out what the type is\n",
    "* It doesn't always do a good job and you might find, while working on the code below, things getting converted into Any for no reason\n",
    "* Sometimes you can fix this by converting things back to String or Double\n",
    "* But, not always. It is better to be safe and specify types always!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f597322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "implicit_data = Array((Jack,34.5,22.7), (Jill,31.3,41.2))\n",
       "explicit_data = Array((Jack,34.5,22.7), (Jill,31.3,41.2))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((Jack,57.2), (Jill,72.5))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Implicit specification\n",
    "val implicit_data = Array(\n",
    "  (\"Jack\", 34.5, 22.7),\n",
    "  (\"Jill\", 31.3, 41.2)\n",
    ")\n",
    "\n",
    "//Explicit specification\n",
    "//Try to do this as often as possible\n",
    "val explicit_data: Array[(String,Double,Double)] = Array(\n",
    "  (\"Jack\", 34.5, 22.7),\n",
    "  (\"Jill\", 31.3, 41.2)\n",
    ")\n",
    "\n",
    "//When doing pattern matching, try to give explicit types in the pattern\n",
    "explicit_data.map(t => t match {\n",
    "    case (name: String, d1: Double, d2: Double) => (name, d1 + d2)\n",
    "    case _ => (\"Missing\",0.0,0.0)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d55591",
   "metadata": {},
   "source": [
    "### Scala help: ClassTag ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cdb6329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "implicit_data = Array((Jack,34.5,22.7), (Jill,31.3,41.2))\n",
       "type_of_implicit_data = Array[scala.Tuple3]\n",
       "type_of_data_of_array_element = scala.Tuple3\n",
       "type_of_data_of_first_element_of_tuple = java.lang.String\n",
       "type_of_data_of_second_element_of_tuple = Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Double"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//If you're confused about the type of a data item\n",
    "//Use the ClassTag function\n",
    "\n",
    "val implicit_data = Array(\n",
    "  (\"Jack\", 34.5, 22.7),\n",
    "  (\"Jill\", 31.3, 41.2)\n",
    ")\n",
    "\n",
    "import scala.reflect.ClassTag\n",
    "val type_of_implicit_data = ClassTag(implicit_data.getClass)\n",
    "val type_of_data_of_array_element = ClassTag(implicit_data(0).getClass)\n",
    "val type_of_data_of_first_element_of_tuple = ClassTag(implicit_data(0)._1.getClass)\n",
    "val type_of_data_of_second_element_of_tuple = ClassTag(implicit_data(0)._2.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55cbd0",
   "metadata": {},
   "source": [
    "### Scala help: Mutable Map ###\n",
    "* Map objects in Scala are like Python dictionaries. They store (key,value) pairs and can access data by key\n",
    "* However, Scala Map objects are immutable unlike Python dictionaries\n",
    "* Scala provides a Mutable version of Map that functions more like Python dictionaries (you can add new keys and can change the value of a key)\n",
    "* Map throws an exception if the key being accessed is not in the Map object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94631361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "departments = Map(Jack -> Sales, Jill -> Pails)\n",
       "jack_dept = Sales\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sales"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Immutable map\n",
    "val departments = Map((\"Jack\",\"Sales\"),(\"Jill\",\"Pails\"))\n",
    "val jack_dept = departments(\"Jack\")\n",
    "\n",
    "//departments(\"Jack\")=\"Accounting\" //Error since departments is immutable\n",
    "//val john_dept = departments(\"John\")//throws an exception since John is not in the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c18f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m_departments = Map(Jill -> Pails, Jack -> Accounting)\n",
       "jack_dept = Sales\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(Jill -> Pails, Jack -> Accounting)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//mutable map\n",
    "import scala.collection._ \n",
    "\n",
    "val m_departments = mutable.Map((\"Jack\",\"Sales\"),(\"Jill\",\"Pails\"))\n",
    "val jack_dept = m_departments(\"Jack\")\n",
    "//val john_dept = m_departments(\"John\")//throws an exception\n",
    "m_departments(\"Jack\") = \"Accounting\" //Can change the department since it is mutable\n",
    "m_departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70bb3594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(Jill -> Pails, Jack -> Accounting, John -> Accounting)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Add John to m_departments\n",
    "m_departments(\"John\") = m_departments.getOrElse(\"John\",\"Accounting\")\n",
    "m_departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e095f4f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "//Add John to departments\n",
    "//Throws an error because departments is immutable\n",
    "//uncoment to run\n",
    "\n",
    "//departments(\"John\") = departments.getOrElse(\"John\",\"Accounting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241aa6df",
   "metadata": {},
   "source": [
    "### Scala help: Converting Any to a specific type ###\n",
    "* Before converting Any to Double or Int, you need to convert it into String\n",
    "* because Any to Int and Any to Double don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a5a1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1 = 342.56\n",
       "x2 = 42\n",
       "x1_double = 342.56\n",
       "x2_int = 42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x1:Any = \"342.56\"\n",
    "val x2:Any = \"42\"\n",
    "\n",
    "val x1_double = x1.toString.toDouble\n",
    "val x2_int = x2.toString.toInt\n",
    "\n",
    "\n",
    "\n",
    "//x1.toDouble  //this won't work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e2252",
   "metadata": {},
   "source": [
    "### Scala help: Scala find function ###\n",
    "* ***find***, applied to a collection, finds the first element of the collection that satisfies the supplied connection\n",
    "* find returns an Option object. You need to de-option it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95539c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample = Array((1,jack,37.2), (2,Jill,23.1))\n",
       "result = 37.2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "37.2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//\n",
    "val sample = Array((1,\"jack\",37.2),(2,\"Jill\",23.1))\n",
    "val result = {\n",
    "    sample.find(t => {\n",
    "                val (_,name:String,_) = t \n",
    "                t._2 == \"jack\"}) \n",
    "    } match {\n",
    "    case Some((_,_,d)) => d\n",
    "    case None => 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4308240",
   "metadata": {},
   "source": [
    "### Spark help: sortBy functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53f9a2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[16] at parallelize at <console>:28\n",
       "sorted_by_3 = Array((2,John,11.7), (2,Jill,23.1), (1,jack,37.2), (4,Qing,42.1))\n",
       "sorted_by_3_descending = Array((4,Qing,42.1), (1,jack,37.2), (2,Jill,23.1), (2,John,11.7))\n",
       "sorted_by_sum = Array((2,John,11.7), (2,Jill,23.1), (1,jack,37.2), (4,Qing,42.1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((2,John,11.7), (2,Jill,23.1), (1,jack,37.2), (4,Qing,42.1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Array((1,\"jack\",37.2),(2,\"Jill\",23.1),(2,\"John\",11.7),(4,\"Qing\",42.1)))\n",
    "val sorted_by_3 = rdd.sortBy(_._3).collect\n",
    "val sorted_by_3_descending = rdd.sortBy(_._3,ascending=false).collect\n",
    "val sorted_by_sum = rdd.sortBy(t => t._1.toDouble+t._3).collect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e268588",
   "metadata": {},
   "source": [
    "### Scala help: maxBy function ###\n",
    "* Scala has a maxBy function that can flexibly find the max of a collection\n",
    "* Spark doesn't have this because functions work within partitions and max requires all partitions\n",
    "* Instead, use reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1db6eec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arr = Array((1,jack,37.2), (2,Jill,23.1), (2,John,11.7), (4,Qing,42.1))\n",
       "highest_3 = (4,Qing,42.1)\n",
       "highest_sum = (4,Qing,42.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4,Qing,42.1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val arr = Array((1,\"jack\",37.2),(2,\"Jill\",23.1),(2,\"John\",11.7),(4,\"Qing\",42.1))\n",
    "val highest_3 = arr.maxBy(_._3)\n",
    "val highest_sum = arr.maxBy(t => t._1 + t._3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a315c52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[32] at parallelize at <console>:30\n",
       "highest_3 = (4,Qing,42.1)\n",
       "highest_sum = (4,Qing,42.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4,Qing,42.1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//maxBy for RDDs\n",
    "val rdd = sc.parallelize(Array((1,\"jack\",37.2),(2,\"Jill\",23.1),(2,\"John\",11.7),(4,\"Qing\",42.1)))\n",
    "\n",
    "val highest_3 = rdd.reduce((a,b) => if (a._3>b._3) a else b)\n",
    "val highest_sum = rdd.reduce((a,b) => if ((a._1+a._3)>(b._1+b._3)) a else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e7669",
   "metadata": {},
   "source": [
    "### STEP 1: Data Preparation ###\n",
    "\n",
    "Write a function ***prepare_data(file)*** that:\n",
    "\n",
    "1. reads a citibike data file into an RDD\n",
    "2. Splits the data on comma\n",
    "3. Removes header lines - the file contains several copies of the header line\n",
    "* ride_id, rideable_type, started_at, ended_at, start_station_name, start_station_id, end_station_name, end_station_id, start_lat, start_lng, end_lat, end_lng, member_casual\n",
    "4. Computes the difference between the bike pickup time and the bike dropoff time and add it as the last element in the rdd\n",
    "5. Save the prepared data in an rdd named ***data_rdd***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63e809d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_rdd = MapPartitionsRDD[38] at map at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "prepare_data: (file: String)org.apache.spark.rdd.RDD[Array[Any]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[38] at map at <console>:41"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(file: String) = {\n",
    "\n",
    "  val header = \"ride_id\"\n",
    "\n",
    "  sc.textFile(file)\n",
    "    .map(_.split(\",\"))\n",
    "    // remove all header lines\n",
    "    .filter(row => !row(0).contains(header))\n",
    "    // keep only rows with proper length (防止 malformed rows)\n",
    "    .filter(row => row.length >= 13)\n",
    "    // add duration\n",
    "    .map(row => {\n",
    "      val start = row(2)  // started_at\n",
    "      val end = row(3)    // ended_at\n",
    "      val duration = time_difference(start, end)\n",
    "      row :+ duration\n",
    "    })\n",
    "}\n",
    "\n",
    "val data_rdd = prepare_data(citibike_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913f0bc",
   "metadata": {},
   "source": [
    "### STEP 2: Report the following statistics using data_rdd ###\n",
    "1. The count of the records (***use count***)\n",
    "2. The average duration that a bike is out (use ***reduce***)\n",
    "3. The number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dee9a1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_count = 76531242\n",
       "avg_duration = 1689.2749273438058\n",
       "numPartitions = 110\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val total_count =   //76531242\n",
    "val avg_duration =   //1689.2749273438058\n",
    "val numPartitions =  //110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d24f7d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_count = 76531242\n",
       "avg_duration = 1689.2749273438053\n",
       "numPartitions = 110\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val total_count = data_rdd.count()\n",
    "\n",
    "val avg_duration = data_rdd.map(row => row.last.toString.toDouble).reduce(_ + _) / total_count.toDouble\n",
    "\n",
    "val numPartitions = data_rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e961b39",
   "metadata": {},
   "source": [
    "### STEP 3: Combine By Key ###\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "Starting with data_rdd, create a (key,value) paired data that contains:\n",
    "\n",
    "(starting station,duration)\n",
    "\n",
    "For example, if the record is:\n",
    "\n",
    "Array(7FB44ACE772F2B5E, classic_bike, 2020-01-16 11:29:00.511, 2020-01-16 11:38:38.211, E 47 St & Park Ave, 6584.12, 5 Ave & E 63 St, 6904.06, 40.755103, -73.974987, 40.766368, -73.971518, member, 577.7)\n",
    "\n",
    "The corresponding (key,value) pair is:\n",
    "\n",
    "(E 47 St & Park Ave,577.7)\n",
    "\n",
    "Store the key,value pairs in a variable named ***station_time_pair***\n",
    "\n",
    "**Part 2**\n",
    "1. Using ***combineByKey*** calculate the average duration a bike is borrowed from each station\n",
    "2. Using ***sortBy*** sort the rdd in descending order\n",
    "3. Report the top ten results (using ***take***, ***foreach***, and ***println***)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adfc6682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lafayette St & Jersey St S,3.0070220633E7)\n",
      "(Amsterdam Ave & W 189 St old,5406588.302)\n",
      "(Propsect Ave & E 151 St,157955.35202380954)\n",
      "(Liberty State Park,150289.825)\n",
      "(,77299.60101684534)\n",
      "(Morris Ave & E 153 St,23464.733022277564)\n",
      "(Southern Blvd & E 172 St,22514.056611127464)\n",
      "(Clinton St & Newark St,22424.762615384618)\n",
      "(Grand Concourse & E 144 St,21538.461101160196)\n",
      "(E 12 St & 4 Av,20832.740200941913)\n",
      "Elapsed time: 56.233019626 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "station_time_pair = MapPartitionsRDD[41] at map at <console>:32\n",
       "combiner = > (Int, Double) = $Lambda$5970/0x0000000802209840@744a5e63\n",
       "merger = > (Int, Double) = $Lambda$5971/0x000000080220a840@469df542\n",
       "mergeAndCombiner = > (Int, Double) = $Lambda$5972/0x000000080220b040@71dca092\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "> (Int, Double) = $Lambda$5972/0x000000080220b040@71dca092"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val station_time_pair = data_rdd.map(row => {\n",
    "  val station = row(4).toString\n",
    "  val duration = row.last.toString.toDouble\n",
    "  (station, duration)\n",
    "})\n",
    "\n",
    "\n",
    "val combiner = (x: Double) => (1, x)   // (count, sum)\n",
    "val merger = (x: (Int, Double), y: Double) => {\n",
    "  val (c, acc) = x\n",
    "  (c + 1, acc + y)\n",
    "}\n",
    "val mergeAndCombiner = (x1: (Int, Double), x2: (Int, Double)) => {\n",
    "  val (c1, acc1) = x1\n",
    "  val (c2, acc2) = x2\n",
    "  (c1 + c2, acc1 + acc2)\n",
    "}\n",
    "\n",
    "\n",
    "timer {\n",
    "  // Combine durations by station\n",
    "  val result = station_time_pair.combineByKey(combiner, merger, mergeAndCombiner)\n",
    "\n",
    "  // Compute average, sort, take top 10\n",
    "  val average = result\n",
    "    .mapValues{ case (count, sum) => sum / count }\n",
    "    .sortBy(_._2, ascending = false)\n",
    "    .take(10)\n",
    "\n",
    "  average.foreach(println)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d48d7",
   "metadata": {},
   "source": [
    "### STEP 4: Hash Partition and Combine By Key ###\n",
    "1. Get the total number of starting stations in the data\n",
    "2. Hash Partition the data into those many partitions and store the result in hash_data\n",
    "3. Run combineByKey again and time the time it takes to return the top ten stations (Elapsed time: 123.860593854 seconds)\n",
    "4. Run it a second time on hash_data, without including the HashPartitioner statement and report the time (Elapsed time: 27.922944971 seconds)\n",
    "5. Explain the three different times (station_time_data, hashing + hash_data, hash_data alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30950f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations = 1863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numStations = 1863\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1863"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numStations = station_time_pair.keys.distinct().count().toInt\n",
    "println(s\"Number of stations = $numStations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2d92ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lafayette St & Jersey St S,3.0070220633E7)\n",
      "(Amsterdam Ave & W 189 St old,5406588.302)\n",
      "(Propsect Ave & E 151 St,157955.35202380954)\n",
      "(Liberty State Park,150289.825)\n",
      "(,77299.60101684532)\n",
      "(Morris Ave & E 153 St,23464.733022277527)\n",
      "(Southern Blvd & E 172 St,22514.056611127475)\n",
      "(Clinton St & Newark St,22424.762615384618)\n",
      "(Grand Concourse & E 144 St,21538.461101160232)\n",
      "(E 12 St & 4 Av,20832.740200941917)\n",
      "Elapsed time: 86.413022711 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hash_data = ShuffledRDD[53] at partitionBy at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[53] at partitionBy at <console>:34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val hash_data = station_time_pair.partitionBy(new HashPartitioner(numStations))\n",
    "timer {\n",
    "  val h_avg_times = hash_data\n",
    "    .combineByKey(combiner, merger, mergeAndCombiner)\n",
    "    .mapValues{case (count, sum) => sum / count}\n",
    "    .sortBy(_._2, ascending = false)\n",
    "    .take(10)\n",
    "\n",
    "  h_avg_times.foreach(println)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88105d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lafayette St & Jersey St S,3.0070220633E7)\n",
      "(Amsterdam Ave & W 189 St old,5406588.302)\n",
      "(Propsect Ave & E 151 St,157955.35202380954)\n",
      "(Liberty State Park,150289.825)\n",
      "(,77299.6010168454)\n",
      "(Morris Ave & E 153 St,23464.733022277564)\n",
      "(Southern Blvd & E 172 St,22514.056611127442)\n",
      "(Clinton St & Newark St,22424.762615384618)\n",
      "(Grand Concourse & E 144 St,21538.4611011603)\n",
      "(E 12 St & 4 Av,20832.740200941917)\n",
      "Elapsed time: 14.954544727 seconds\n"
     ]
    }
   ],
   "source": [
    "//Second run, without creating hash_data since it is already created in the previous cell\n",
    "timer {\n",
    "  val h_avg_times2 = hash_data\n",
    "    .combineByKey(combiner, merger, mergeAndCombiner)\n",
    "    .mapValues{case (count, sum) => sum / count}\n",
    "    .sortBy(_._2, ascending = false)\n",
    "    .take(10)\n",
    "\n",
    "  h_avg_times2.foreach(println)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584eb82",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "\n",
    "In the first run using station_time_pair, Spark performs a full shuffle because the data is not pre-partitioned. This causes high communication overhead when grouping by starting station.\n",
    "In the second run, we applied a HashPartitioner with the number of unique stations. This reduces shuffle cost because records for the same key are already colocated on the same executors before combineByKey, improving performance.\n",
    "In the third run, we reused the already-partitioned hash_data without calling HashPartitioner again. Since the data was already distributed by key, Spark avoided an additional shuffle, leading to the fastest time among the three.\n",
    "\n",
    "***In summary, no-partition > hash partition > reusing hash partition, because reducing shuffle significantly speeds up the aggregation.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525eb3c",
   "metadata": {},
   "source": [
    "### STEP 5: Repartition the data using a custom partitioner ###\n",
    "In this step, you will repartition the data as follows:\n",
    "1. Each bike trip has a starting station and that starting station is in a New York City zipcode\n",
    "2. We don't know the zipcode but, using the ***find_zipcode** function, you can get it given the latitude and longitude of the starting station\n",
    "3. We'll construct a new key,value paired data as follows:\n",
    "    * (starting station, duration, (lat, lon))\n",
    "4. You'll then write a custom partitoner that uses this data to allocate each record to its zipcode partition\n",
    "5. The function ***read_zipcode_polygons*** reads zipcode polygons and returns data in the format explained above \n",
    "6. In the custom partitioner, find the zipcode for each record and use the id of the zipcode as the partition number\n",
    "\n",
    "**NOTES**\n",
    "\n",
    "* Custom partitioners only have access to the key of a key value pair. Since you need to partition the data using the latitude and longitude, change the key to the latitude and longitude before passing it to the partitioner\n",
    "* The final form of the data in each custom partition should be (starting station, (duration, (lat, lon))). We want the data to be stored with the starting station as the key. Use a map transformation after partitioning the data\n",
    "* The data contains a glitch that you will need to handle:\n",
    "    * the latitude and longitude sometimes contains bad data (characters or an empty string). Use the ***get_lat_lon*** function to get the latitude and longitude as Doubles. Bad values are converted to 0.0. Therefore, after you've constructed the (starting station, duration, (lat, lon)), filter out any trips that have either latitude or longitude equal to 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51242486",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "1. Create a ***station_latlon_rdd*** from ***data_rdd** with the format specified above (see the notes above as well!)\n",
    "2. Get the zipcode polygons and save them in a variable ***polygons***. Note that though the function returns an rdd, you should run ***collect*** and save this as a Scala object (to avoid serialization errors!)\n",
    "3. Write a Custom Partitioner ***zipcodePartitioner*** that takes the polygons as an initializer (i.e., it is an argument to the class definition) and does the following:\n",
    "    * sets the number of partitions to the number of zipcodes. Note that .length will not work because the partitioner works locally and cannot calculate the global length of a scala object. Instead, use ***distinct*** and ***size*** to get the number of zipcodes\n",
    "    * write getPartition \n",
    "        * Use Scala's find function [Scala find](https://www.geeksforgeeks.org/scala/scala-iterator-find-method-with-example/) to apply ***find_zipcode*** to each key (the key should be (lat, lon)) to find the zipcode. If the zipcode is found, return the id of the zipcode (the 0, 1, ... - second element in the polygon object). Otherwise, return 0 (if we can't find the zipcode, we'll just add the trip to the 0th partition). \n",
    "        * Note that find returns an Option object. You'll need to remove the Some or the None. Use ***match*** to do this\n",
    "        * the value returned by find after removing Some or None is the partition number and that's what getPartition returns\n",
    "4. Do the partitioning\n",
    "    * remember to use map to convert the key to (lat,lon)\n",
    "    * and remember to use map on the result to convert the data into (station,(duration,(lat,lon)))\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3085e9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_latlon_rdd = MapPartitionsRDD[71] at filter at <console>:40\n",
       "polygons = Array((11372,0,Array((-73.86942457284177,40.74915687096788), (-73.89143129977276,40.74684466041932), (-73.89507143240859,40.746465470812154), (-73.8961873786782,40.74850942518088), (-73.8958395418514,40.74854687570604), (-73.89525242774397,40.748306609450246), (-73.89654041085562,40.75054199814359), (-73.89579868613829,40.75061972133262), (-73.89652230661434,40.75438879610903), (-73.88164812188481,40.75595161704187), (-73.87221855882478,40.75694324806748), (-73.87167992356792,40.75398717439604), (-73.8720704651389,40.753862007052064), (-73.86942457284177,40.74915687096788))), (11004,1,Array((-73.711329...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((11372,0,Array((-73.86942457284177,40.74915687096788), (-73.89143129977276,40.74684466041932), (-73.89507143240859,40.746465470812154), (-73.8961873786782,40.74850942518088), (-73.8958395418514,40.74854687570604), (-73.89525242774397,40.748306609450246), (-73.89654041085562,40.75054199814359), (-73.89579868613829,40.75061972133262), (-73.89652230661434,40.75438879610903), (-73.88164812188481,40.75595161704187), (-73.87221855882478,40.75694324806748), (-73.87167992356792,40.75398717439604), (-73.8720704651389,40.753862007052064), (-73.86942457284177,40.74915687096788))), (11004,1,Array((-73.711329..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val station_latlon_rdd = data_rdd.map(row => {\n",
    "  val station = row(4).toString\n",
    "  val duration = row(13).asInstanceOf[Double]\n",
    "\n",
    "  // get_lat_lon expects (lat, lon) as two inputs!\n",
    "  val (lat, lon) = get_lat_lon(row(8), row(9))  // returns (Double, Double)\n",
    "\n",
    "  (station, duration, (lat, lon))\n",
    "}).filter{ case (_,_,(lat,lon)) => lat != 0.0 && lon != 0.0 }\n",
    "val polygons = read_zipcode_polygons(zipcode_geojson_file).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27b8235d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ZipcodePartitioner\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.Partitioner\n",
    "\n",
    "class ZipcodePartitioner(polygons: Array[(String, Int, Array[(Double, Double)])])\n",
    "  extends org.apache.spark.Partitioner {\n",
    "\n",
    "  override val numPartitions: Int = polygons.map(_._2).distinct.size\n",
    "\n",
    "  def findZip(lat: Double, lon: Double): Int = {\n",
    "    polygons.find { case (zip, id, coords) =>\n",
    "      find_zipcode((zip, id, coords), (lon, lat)) != -1\n",
    "    } match {\n",
    "      case Some((_, id, _)) => id\n",
    "      case None => 0\n",
    "    }\n",
    "  }\n",
    "\n",
    "  override def getPartition(key: Any): Int = {\n",
    "    key match {\n",
    "      case (lat: Double, lon: Double) => findZip(lat, lon)\n",
    "      case _ => 0\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3106dfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_partition_rdd = MapPartitionsRDD[87] at map at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[87] at map at <console>:42"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val custom_partition_rdd =\n",
    "  station_latlon_rdd\n",
    "    .map { case (station, duration, (lat, lon)) =>\n",
    "      ((lat, lon), (station, duration))\n",
    "    }\n",
    "    .partitionBy(new ZipcodePartitioner(polygons))\n",
    "    .map { case ((lat, lon), (station, duration)) =>\n",
    "      (station, (duration, (lat, lon)))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca719d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Test: You should get 112 partitions with non-zero elements (not every zipcode has a citibike station!)\n",
    "custom_partition_rdd.mapPartitions(iter => Array(iter.size).iterator).filter(v => v!=0).count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3137347",
   "metadata": {},
   "source": [
    "### STEP 6: Find the most active stations for each zipcode ###\n",
    "* Write a function ***mostActiveStationPerZip*** that returns the most active station in each zipcode\n",
    "* Since the data for each zipcode is in its own partition the summing up can be done separately on each partition\n",
    "* You can't use byKey operations because they ALWAYS end up globally and involve a shuffle operation\n",
    "* Instead, since the reduce operation needs to be done on each partition separatelyv use  ***mapPartitionswithIndex***, which works on each partition separately \n",
    "* Once we've found the most active, you will need to lookup the polygons object to match the partition number with the zipcode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f537daa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostActiveStationPerZip: (rdd: org.apache.spark.rdd.RDD[(String, (Double, (Double, Double)))])org.apache.spark.rdd.RDD[(Int, (String, Double))]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.mutable\n",
    "\n",
    "// Computes the most active (highest total duration) station per partition.\n",
    "def mostActiveStationPerZip(\n",
    "  rdd: RDD[(String, (Double, (Double, Double)))]\n",
    "): RDD[(Int, (String, Double))] = {\n",
    "\n",
    "  rdd.mapPartitionsWithIndex { (pid, iter) =>\n",
    "\n",
    "    // Create a mutable Map: station -> totalDuration\n",
    "    val stationTotals = mutable.Map[String, Double]()\n",
    "\n",
    "    // Aggregate duration per station\n",
    "    iter.foreach { case (station, (duration, (lat, lon))) =>\n",
    "      val current = stationTotals.getOrElse(station, 0.0)\n",
    "      stationTotals(station) = current + duration\n",
    "    }\n",
    "\n",
    "    // Return max station from this partition\n",
    "    if (stationTotals.nonEmpty) {\n",
    "      val (maxStation, maxDuration) = stationTotals.maxBy(_._2)\n",
    "      Iterator((pid, (maxStation, maxDuration)))\n",
    "    } else {\n",
    "      Iterator.empty\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f58c8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostActives = MapPartitionsRDD[91] at mapPartitionsWithIndex at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[91] at mapPartitionsWithIndex at <console>:41"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mostActives = mostActiveStationPerZip(custom_partition_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed32a1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,(South St & Gouverneur Ln,2.2749358650000027E8))\n",
      "(21,(Lewis Ave & Madison St,2.350225190540018E8))\n",
      "(25,(Hancock St & Bedford Ave,2.5918673494900116E8))\n",
      "(27,(Fulton St & Utica Ave,3.386516076930045E8))\n",
      "(29,(Brooklyn Ave & Prospect Pl,839592.471000001))\n"
     ]
    }
   ],
   "source": [
    "// mostActives.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66134fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_result = MapPartitionsRDD[94] at map at <console>:37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[94] at map at <console>:37"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_result = mostActives.map{ case (pid, (station, duration)) =>\n",
    "  val zip = polygons.map{ case (zip, id, _) => (id, zip) }.toMap.getOrElse(pid, \"Unknown\")\n",
    "  (zip, pid, (station, duration))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d77266a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((11372,0,(South St & Gouverneur Ln,2.2749358649999747E8)), (11221,21,(Lewis Ave & Madison St,2.350225190539995E8)), (11216,25,(Hancock St & Bedford Ave,2.5918673494900003E8)), (11233,27,(Fulton St & Utica Ave,3.3865160769300246E8)), (11213,29,(Brooklyn Ave & Prospect Pl,839592.4710000005)), (11225,31,(Franklin Ave & Empire Blvd,3.3806142728299993E8)), (11218,32,(Ocean Pkwy & Church Ave,3.1160455834999934E7)), (11226,33,(Parade Pl & Crooke Ave,1.2468014570000012E8)), (11230,36,(E 14 St & 1 Ave,1345.643)), (10467,41,(Jerome Ave & E Mosholu Parkway S,1.1223364132000007E7)), (10463,42,(Bailey Ave & W 193 St,3898469.234999996)), (10468,46,(Jerome Ave & W 193 St,2.141050999199991E7)), (10458,48,(Marion Ave & Mosholu Pkwy,3.826985452299983E..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fd5e5-2f18-418f-816d-8e8f8e7d9aa2",
   "metadata": {},
   "source": [
    "#  Summary\n",
    "\n",
    "This notebook demonstrates advanced Spark and data engineering techniques:\n",
    "\n",
    "- Loading large datasets from Google Cloud Storage  \n",
    "- Distributed data preparation & time computations  \n",
    "- Aggregations using RDD API (`combineByKey`, `reduce`)  \n",
    "- Evaluating partitioning strategies and shuffle behavior  \n",
    "- Implementing a custom geospatial partitioner for zipcode-based locality  \n",
    "\n",
    "The project highlights how proper partitioning and geospatial awareness can significantly improve performance in distributed systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
